{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dfce22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 17:13:00.965159: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-13 17:13:01.092164: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-13 17:13:01.092272: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-13 17:13:01.100672: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-13 17:13:01.135742: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-13 17:13:01.136754: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-13 17:13:05.404500: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3a689aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from tensorflow_examples.models.pix2pix import pix2pix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e2fb52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 498 files belonging to 1 classes.\n",
      "Found 498 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "dataset_directory = 'Image Dataset'  \n",
    "batch_size = 32  \n",
    "image_size = (256,256)  \n",
    "dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    dataset_directory,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    image_size=image_size\n",
    ")\n",
    "dataset_directory = 'Inverted Images Dataset'  \n",
    "batch_size = 32\n",
    "image_size = (256,256)  \n",
    "rotated_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    dataset_directory,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    image_size=image_size\n",
    ")\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "#dataset = dataset.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "#rotated_dataset = dataset.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "794b76e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(image):\n",
    "    img = tf.cast(img, dtype=tf.float32)\n",
    "    return (img / 127.5) - 1.0\n",
    "def preprocess_image(image):\n",
    "    image = normalize(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9c3d2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 64\n",
    "BATCH_SIZE = 32\n",
    "IMG_WIDTH = 256\n",
    "IMG_HEIGHT = 256\n",
    "OUTPUT_CHANNELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9a91c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def filter_batch_size(batch, batch_size=32):\n",
    "    return tf.shape(batch)[0] == batch_size\n",
    "filtered_dataset = dataset.filter(lambda x, y: filter_batch_size(x, batch_size=32))\n",
    "filtered_rotated_dataset = rotated_dataset.filter(lambda x, y: filter_batch_size(x, batch_size=32))\n",
    "generator_g = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n",
    "generator_f = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n",
    "\n",
    "discriminator_x = pix2pix.discriminator(norm_type='instancenorm', target=False)\n",
    "discriminator_y = pix2pix.discriminator(norm_type='instancenorm', target=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f18676e",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37216f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_ParallelMapDataset element_spec=(TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>\n",
      "Shape after squeeze: (32, 256, 256, 3)\n",
      "Shape after squeeze: (32, 256, 256, 3)\n",
      "Shape after squeeze: (32, 256, 256, 3)\n",
      "Shape after squeeze: (32, 256, 256, 3)\n",
      "Shape after squeeze: (32, 256, 256, 3)\n",
      "Shape after squeeze: (32, 256, 256, 3)\n",
      "Shape after squeeze: (32, 256, 256, 3)\n",
      "Shape after squeeze: (32, 256, 256, 3)\n",
      "Shape after squeeze: (32, 256, 256, 3)\n",
      "Shape after squeeze: (32, 256, 256, 3)\n",
      "Shape after squeeze: (32, 256, 256, 3)\n",
      "Shape after squeeze: (32, 256, 256, 3)\n",
      "Shape after squeeze: (32, 256, 256, 3)\n",
      "Shape after squeeze: (32, 256, 256, 3)\n",
      "Shape after squeeze: (32, 256, 256, 3)\n",
      "Shape after squeeze: (18, 256, 256, 3)\n",
      "Shape after squeeze: (32, 256, 256, 3)\n",
      "Shape after squeeze: (32, 256, 256, 3)\n",
      "Shape after squeeze: (32, 256, 256, 3)\n",
      "Shape after squeeze: (32, 256, 256, 3)\n",
      "Shape after squeeze: (32, 256, 256, 3)\n",
      "Shape after squeeze: (32, 256, 256, 3)\n",
      "Shape after squeeze: (32, 256, 256, 3)\n",
      "Shape after squeeze: (32, 256, 256, 3)\n",
      "Shape after squeeze: (32, 256, 256, 3)\n",
      "Shape after squeeze: (32, 256, 256, 3)\n",
      "Shape after squeeze: (32, 256, 256, 3)\n",
      "Shape after squeeze: (32, 256, 256, 3)\n",
      "Shape after squeeze: (32, 256, 256, 3)\n",
      "Shape after squeeze: (32, 256, 256, 3)\n",
      "Shape after squeeze: (32, 256, 256, 3)\n",
      "Shape after squeeze: (18, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_image(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    return image, label\n",
    "\n",
    "# Apply the preprocess function to the datasets\n",
    "dataset = dataset.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "rotated_dataset = rotated_dataset.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Batch and prefetch the datasets with drop_remainder=True\n",
    "#dataset = dataset.batch(BATCH_SIZE, drop_remainder=False).prefetch(tf.data.AUTOTUNE)\n",
    "#rotated_dataset = rotated_dataset.batch(BATCH_SIZE, drop_remainder=False).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Define the models\n",
    "generator_g = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n",
    "generator_f = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n",
    "\n",
    "print(dataset)\n",
    "# Process each batch in the dataset\n",
    "for batch in dataset:\n",
    "    upright_images = batch[0]  # This extracts the image tensor from the batch\n",
    "    upright_images = tf.squeeze(upright_images)\n",
    "    print(\"Shape after squeeze:\", upright_images.shape)\n",
    "    if upright_images.shape[-1] == 3:\n",
    "        to_flipped = generator_g(upright_images)\n",
    "    else:\n",
    "        print(\"Incorrect image shape\")\n",
    "\n",
    "for batch in rotated_dataset:\n",
    "    rotated_images = batch[0]\n",
    "    rotated_images = tf.squeeze(rotated_images)\n",
    "    print(\"Shape after squeeze:\", rotated_images.shape)\n",
    "    if rotated_images.shape[-1] == 3:\n",
    "        to_upright = generator_f(rotated_images)\n",
    "    else:\n",
    "        print(\"Incorrect image shape\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cde6158",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA = 10\n",
    "loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "def discriminator_loss(real, generated):\n",
    "    real_loss = loss_obj(tf.ones_like(real), real)\n",
    "    generated_loss = loss_obj(tf.zeros_like(generated), generated)\n",
    "    total_disc_loss = real_loss + generated_loss\n",
    "    return total_disc_loss * 0.5\n",
    "def generator_loss(generated):\n",
    "    return loss_obj(tf.ones_like(generated), generated)\n",
    "def calc_cycle_loss(real_image, cycled_image):\n",
    "    loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n",
    "    return LAMBDA * loss1\n",
    "def identity_loss(real_image, same_image):\n",
    "    loss = tf.reduce_mean(tf.abs(real_image - same_image))\n",
    "    return LAMBDA * 0.1 * loss\n",
    "generator_g_optimizer = tf.keras.optimizers.legacy.Adam(2e-4, beta_1=0.5)\n",
    "generator_f_optimizer = tf.keras.optimizers.legacy.Adam(2e-4, beta_1=0.5)\n",
    "\n",
    "discriminator_x_optimizer = tf.keras.optimizers.legacy.Adam(2e-4, beta_1=0.5)\n",
    "discriminator_y_optimizer = tf.keras.optimizers.legacy.Adam(2e-4, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f8deb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(generator_g=generator_g,\n",
    "                           generator_f=generator_f,\n",
    "                           discriminator_x=discriminator_x,\n",
    "                           discriminator_y=discriminator_y,\n",
    "                           generator_g_optimizer=generator_g_optimizer,\n",
    "                           generator_f_optimizer=generator_f_optimizer,\n",
    "                           discriminator_x_optimizer=discriminator_x_optimizer,\n",
    "                           discriminator_y_optimizer=discriminator_y_optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "407a7c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 500\n",
    "def generate_images(model, test_input):\n",
    "    prediction = model(test_input[0])\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    display_list = [test_input[0], prediction[0]]\n",
    "    title = ['Input Image', 'Predicted Image']\n",
    "    for i in range(2):\n",
    "        plt.subplot(1, 2, i+1)\n",
    "        plt.title(title[i])\n",
    "        # getting the pixel values between [0, 1] to plot it.\n",
    "        plt.imshow(display_list[i] * 0.5 + 0.5)\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "818918bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(real_x, real_y):\n",
    "  # persistent is set to True because the tape is used more than\n",
    "  # once to calculate the gradients.\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        # Generator G translates X -> Y\n",
    "        # Generator F translates Y -> X.\n",
    "\n",
    "        fake_y = generator_g(real_x, training=True)\n",
    "        cycled_x = generator_f(fake_y, training=True)\n",
    "\n",
    "        fake_x = generator_f(real_y, training=True)\n",
    "        cycled_y = generator_g(fake_x, training=True)\n",
    "\n",
    "        # same_x and same_y are used for identity loss.\n",
    "        same_x = generator_f(real_x, training=True)\n",
    "        same_y = generator_g(real_y, training=True)\n",
    "\n",
    "        disc_real_x = discriminator_x(real_x, training=True)\n",
    "        disc_real_y = discriminator_y(real_y, training=True)\n",
    "\n",
    "        disc_fake_x = discriminator_x(fake_x, training=True)\n",
    "        disc_fake_y = discriminator_y(fake_y, training=True)\n",
    "\n",
    "        # calculate the loss\n",
    "        gen_g_loss = generator_loss(disc_fake_y)\n",
    "        gen_f_loss = generator_loss(disc_fake_x)\n",
    "\n",
    "        total_cycle_loss = calc_cycle_loss(real_x, cycled_x) + calc_cycle_loss(real_y, cycled_y)\n",
    "\n",
    "        # Total generator loss = adversarial loss + cycle loss\n",
    "        total_gen_g_loss = gen_g_loss + total_cycle_loss + identity_loss(real_y, same_y)\n",
    "        total_gen_f_loss = gen_f_loss + total_cycle_loss + identity_loss(real_x, same_x)\n",
    "\n",
    "        disc_x_loss = discriminator_loss(disc_real_x, disc_fake_x)\n",
    "        disc_y_loss = discriminator_loss(disc_real_y, disc_fake_y)\n",
    "\n",
    "  # Calculate the gradients for generator and discriminator\n",
    "    generator_g_gradients = tape.gradient(total_gen_g_loss, \n",
    "                                        generator_g.trainable_variables)\n",
    "    generator_f_gradients = tape.gradient(total_gen_f_loss, \n",
    "                                        generator_f.trainable_variables)\n",
    "\n",
    "    discriminator_x_gradients = tape.gradient(disc_x_loss, \n",
    "                                            discriminator_x.trainable_variables)\n",
    "    discriminator_y_gradients = tape.gradient(disc_y_loss, \n",
    "                                            discriminator_y.trainable_variables)\n",
    "\n",
    "  # Apply the gradients to the optimizer\n",
    "    generator_g_optimizer.apply_gradients(zip(generator_g_gradients, \n",
    "                                            generator_g.trainable_variables))\n",
    "\n",
    "    generator_f_optimizer.apply_gradients(zip(generator_f_gradients, \n",
    "                                            generator_f.trainable_variables))\n",
    "\n",
    "    discriminator_x_optimizer.apply_gradients(zip(discriminator_x_gradients,\n",
    "                                                discriminator_x.trainable_variables))\n",
    "\n",
    "    discriminator_y_optimizer.apply_gradients(zip(discriminator_y_gradients,\n",
    "                                                discriminator_y.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b270c5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".."
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    n = 0\n",
    "    for image_x, image_y in zip(dataset, rotated_dataset):\n",
    "        # Assuming image_x and image_y are tuples of (image, label), \n",
    "        # and you only need the image for training\n",
    "        train_step(image_x[0], image_y[0])\n",
    "        if n % 10 == 0:\n",
    "            print('.', end='')\n",
    "        n += 1\n",
    "    clear_output(wait=True)\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print('Saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n",
    "    print('Time taken for epoch {} is {} sec\\n'.format(epoch + 1, time.time()-start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c42ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for inp in dataset.take(5):\n",
    "    generate_images(generator_f, inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3712c615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(model, test_input):\n",
    "    # Assuming test_input is a tuple where the first item is the input data\n",
    "    input_data = test_input[0]\n",
    "    predictions = model(input_data, training=False)\n",
    "\n",
    "    # Check the dimensions of input_data\n",
    "    if input_data.ndim == 4:\n",
    "        # Expected shape for a batch of images: (batch_size, height, width, channels)\n",
    "        batch_size = input_data.shape[0]\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected shape of input_data: {input_data.shape}\")\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        plt.figure(figsize=(10, 10))\n",
    "\n",
    "        # Extract the input image and the corresponding prediction\n",
    "        input_image = input_data[i].numpy() if hasattr(input_data[i], 'numpy') else input_data[i]\n",
    "        predicted_image = predictions[i].numpy() if hasattr(predictions[i], 'numpy') else predictions[i]\n",
    "\n",
    "        # Check if the image is in the correct shape\n",
    "        if input_image.ndim != 3 or input_image.shape[-1] != 3:\n",
    "            raise ValueError(f\"Input image shape {input_image.shape} is not valid for display\")\n",
    "\n",
    "        if predicted_image.ndim != 3 or predicted_image.shape[-1] != 3:\n",
    "            raise ValueError(f\"Predicted image shape {predicted_image.shape} is not valid for display\")\n",
    "\n",
    "        # Display the input and predicted images\n",
    "        for j, image in enumerate([input_image, predicted_image]):\n",
    "            plt.subplot(1, 2, j+1)\n",
    "            plt.title(['Input Image', 'Predicted Image'][j])\n",
    "            plt.imshow(image * 0.5 + 0.5)  # Normalize pixel values\n",
    "            plt.axis('off')\n",
    "\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24342ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
